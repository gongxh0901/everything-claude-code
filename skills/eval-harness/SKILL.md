---
name: eval-harness
description: Claude Code 会话的正式评估框架，实现评估驱动开发（EDD）原则
tools: Read, Write, Edit, Bash, Grep, Glob
---

# 评估框架技能

Claude Code 会话的正式评估框架，实现评估驱动开发（EDD）原则。

## 理念

评估驱动开发将评估视为"AI 开发的单元测试"：
- 在实现之前定义预期行为
- 开发过程中持续运行评估
- 每次变更跟踪回归
- 使用 pass@k 指标衡量可靠性

## 评估类型

### 能力评估
测试 Claude 能否做到之前做不到的事：
```markdown
[能力评估: feature-name]
任务: Claude 应完成什么的描述
成功标准:
  - [ ] 标准 1
  - [ ] 标准 2
  - [ ] 标准 3
预期输出: 预期结果的描述
```

### 回归评估
确保变更不会破坏现有功能：
```markdown
[回归评估: feature-name]
基线: SHA 或检查点名称
测试:
  - existing-test-1: 通过/失败
  - existing-test-2: 通过/失败
  - existing-test-3: 通过/失败
结果: X/Y 通过（之前 Y/Y）
```

## 评分器类型

### 1. 基于代码的评分器
使用代码进行确定性检查：
```bash
# 检查文件是否包含预期模式
grep -q "export function handleAuth" src/auth.ts && echo "通过" || echo "失败"

# 检查测试是否通过
npm test -- --testPathPattern="auth" && echo "通过" || echo "失败"

# 检查构建是否成功
npm run build && echo "通过" || echo "失败"
```

### 2. 模型评分器
使用 Claude 评估开放式输出：
```markdown
[模型评分器提示]
评估以下代码变更：
1. 它解决了陈述的问题吗？
2. 结构良好吗？
3. 边界情况处理了吗？
4. 错误处理适当吗？

分数: 1-5（1=差，5=优秀）
原因: [解释]
```

### 3. 人工评分器
标记需要人工审查：
```markdown
[需要人工审查]
变更: 变更内容的描述
原因: 为什么需要人工审查
风险级别: 低/中/高
```

## 指标

### pass@k
"k 次尝试中至少成功一次"
- pass@1: 首次尝试成功率
- pass@3: 3 次尝试内成功
- 典型目标: pass@3 > 90%

### pass^k
"所有 k 次尝试都成功"
- 更高的可靠性标准
- pass^3: 连续 3 次成功
- 用于关键路径

## 评估工作流

### 1. 定义（编码前）
```markdown
## 评估定义: feature-xyz

### 能力评估
1. 能创建新用户账户
2. 能验证邮箱格式
3. 能安全地哈希密码

### 回归评估
1. 现有登录仍然工作
2. 会话管理未变
3. 登出流程完整

### 成功指标
- 能力评估 pass@3 > 90%
- 回归评估 pass^3 = 100%
```

### 2. 实现
编写代码以通过定义的评估。

### 3. 评估
```bash
# 运行能力评估
[运行每个能力评估，记录通过/失败]

# 运行回归评估
npm test -- --testPathPattern="existing"

# 生成报告
```

### 4. 报告
```markdown
评估报告: feature-xyz
========================

能力评估:
  create-user:     通过 (pass@1)
  validate-email:  通过 (pass@2)
  hash-password:   通过 (pass@1)
  总体:            3/3 通过

回归评估:
  login-flow:      通过
  session-mgmt:    通过
  logout-flow:     通过
  总体:            3/3 通过

指标:
  pass@1: 67% (2/3)
  pass@3: 100% (3/3)

状态: 准备审查
```

## 集成模式

### 实现前
```
/eval define feature-name
```
在 `.claude/evals/feature-name.md` 创建评估定义文件

### 实现中
```
/eval check feature-name
```
运行当前评估并报告状态

### 实现后
```
/eval report feature-name
```
生成完整评估报告

## 评估存储

在项目中存储评估：
```
.claude/
  evals/
    feature-xyz.md      # 评估定义
    feature-xyz.log     # 评估运行历史
    baseline.json       # 回归基线
```

## 最佳实践

1. **编码前定义评估** - 强制清晰思考成功标准
2. **频繁运行评估** - 尽早捕获回归
3. **跟踪 pass@k 随时间变化** - 监控可靠性趋势
4. **尽可能使用代码评分器** - 确定性 > 概率性
5. **安全审查需人工** - 永远不要完全自动化安全检查
6. **保持评估快速** - 慢的评估不会被运行
7. **评估随代码版本化** - 评估是一等工件

## 示例：添加认证

```markdown
## 评估: add-authentication

### 阶段 1: 定义（10 分钟）
能力评估:
- [ ] 用户可以用邮箱/密码注册
- [ ] 用户可以用有效凭证登录
- [ ] 无效凭证被拒绝并显示正确错误
- [ ] 会话在页面刷新后保持
- [ ] 登出清除会话

回归评估:
- [ ] 公共路由仍可访问
- [ ] API 响应未变
- [ ] 数据库模式兼容

### 阶段 2: 实现（时间不定）
[编写代码]

### 阶段 3: 评估
运行: /eval check add-authentication

### 阶段 4: 报告
评估报告: add-authentication
==============================
能力: 5/5 通过 (pass@3: 100%)
回归: 3/3 通过 (pass^3: 100%)
状态: 可以发布
```
